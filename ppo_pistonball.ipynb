{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install SuperSuit==3.6.0\n",
        "# pettingzoo==1.22.3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzl8da5nCRAt",
        "outputId": "c4f2c89b-432e-455c-b2cf-49dc4f6b82ff"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting SuperSuit==3.6.0\n",
            "  Downloading SuperSuit-3.6.0-py3-none-any.whl (68 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.2/68.2 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pettingzoo>=1.21.0\n",
            "  Downloading PettingZoo-1.22.3-py3-none-any.whl (816 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m816.1/816.1 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gym>=0.26.0\n",
            "  Downloading gym-0.26.2.tar.gz (721 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m721.7/721.7 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pymunk==6.2.1\n",
            "  Downloading pymunk-6.2.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (979 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m979.5/979.5 kB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tinyscaler>=1.0.4\n",
            "  Downloading tinyscaler-1.2.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (492 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m492.7/492.7 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pygame==2.1.2\n",
            "  Downloading pygame-2.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.9/21.9 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from pymunk==6.2.1->SuperSuit==3.6.0) (1.15.1)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym>=0.26.0->SuperSuit==3.6.0) (1.24.3)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym>=0.26.0->SuperSuit==3.6.0) (0.0.8)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym>=0.26.0->SuperSuit==3.6.0) (2.2.1)\n",
            "Collecting gymnasium>=0.26.0\n",
            "  Downloading gymnasium-0.28.1-py3-none-any.whl (925 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m925.5/925.5 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.15.0->pymunk==6.2.1->SuperSuit==3.6.0) (2.21)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.26.0->pettingzoo>=1.21.0->SuperSuit==3.6.0) (4.5.0)\n",
            "Collecting jax-jumpy>=1.0.0\n",
            "  Downloading jax_jumpy-1.0.0-py3-none-any.whl (20 kB)\n",
            "Collecting farama-notifications>=0.0.1\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.26.2-py3-none-any.whl size=827652 sha256=b19312c93fcf747a0c4199d764290c772949418891edcc46476e416e0166a1c2\n",
            "  Stored in directory: /root/.cache/pip/wheels/b9/22/6d/3e7b32d98451b4cd9d12417052affbeeeea012955d437da1da\n",
            "Successfully built gym\n",
            "Installing collected packages: farama-notifications, tinyscaler, pygame, jax-jumpy, gym, pymunk, gymnasium, pettingzoo, SuperSuit\n",
            "  Attempting uninstall: pygame\n",
            "    Found existing installation: pygame 2.3.0\n",
            "    Uninstalling pygame-2.3.0:\n",
            "      Successfully uninstalled pygame-2.3.0\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dopamine-rl 4.0.6 requires gym<=0.25.2, but you have gym 0.26.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed SuperSuit-3.6.0 farama-notifications-0.0.4 gym-0.26.2 gymnasium-0.28.1 jax-jumpy-1.0.0 pettingzoo-1.22.3 pygame-2.1.2 pymunk-6.2.1 tinyscaler-1.2.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmYb-QICCDU3",
        "outputId": "45685de1-33ca-4b2c-af53-68f9b49fbd31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training episode 0\n",
            "Episodic Return: 27.027027130126953\n",
            "Episode Length: 124\n",
            "\n",
            "Value Loss: 100.65240478515625\n",
            "Policy Loss: 6.09538459777832\n",
            "Old Approx KL: 0.007599520031362772\n",
            "Approx KL: 0.0019281357526779175\n",
            "Clip Fraction: 0.03463319090441761\n",
            "Explained Variance: -0.0035125017166137695\n",
            "\n",
            "-------------------------------------------\n",
            "\n",
            "Training episode 1\n",
            "Episodic Return: 40.937660217285156\n",
            "Episode Length: 124\n",
            "\n",
            "Value Loss: 48.53568649291992\n",
            "Policy Loss: 6.422234058380127\n",
            "Old Approx KL: 0.03062640130519867\n",
            "Approx KL: 0.002411864697933197\n",
            "Clip Fraction: 0.010327635348862054\n",
            "Explained Variance: -0.0022162199020385742\n",
            "\n",
            "-------------------------------------------\n",
            "\n",
            "Training episode 2\n",
            "Episodic Return: 29.566410064697266\n",
            "Episode Length: 124\n",
            "\n",
            "Value Loss: 174.28431701660156\n",
            "Policy Loss: -15.073929786682129\n",
            "Old Approx KL: -0.0025420337915420532\n",
            "Approx KL: 0.0011313557624816895\n",
            "Clip Fraction: 0.011217948717948718\n",
            "Explained Variance: -0.004194140434265137\n",
            "\n",
            "-------------------------------------------\n",
            "\n",
            "Training episode 3\n",
            "Episodic Return: 88.7000503540039\n",
            "Episode Length: 113\n",
            "\n",
            "Value Loss: 37.0748291015625\n",
            "Policy Loss: 5.622929096221924\n",
            "Old Approx KL: 0.0019569755531847477\n",
            "Approx KL: 1.9848346710205078e-05\n",
            "Clip Fraction: 0.008391203703703705\n",
            "Explained Variance: -0.005500674247741699\n",
            "\n",
            "-------------------------------------------\n",
            "\n",
            "Training episode 4\n",
            "Episodic Return: 8.533215522766113\n",
            "Episode Length: 124\n",
            "\n",
            "Value Loss: 64.62152862548828\n",
            "Policy Loss: 9.67622184753418\n",
            "Old Approx KL: 0.0031542826909571886\n",
            "Approx KL: 0.00046108910464681685\n",
            "Clip Fraction: 0.027332621093234446\n",
            "Explained Variance: 0.009614944458007812\n",
            "\n",
            "-------------------------------------------\n",
            "\n",
            "Training episode 5\n",
            "Episodic Return: -5.192291259765625\n",
            "Episode Length: 124\n",
            "\n",
            "Value Loss: 73.22147369384766\n",
            "Policy Loss: -3.1849193572998047\n",
            "Old Approx KL: -0.0015121499309316278\n",
            "Approx KL: 0.00018581499170977622\n",
            "Clip Fraction: 0.013888888888888888\n",
            "Explained Variance: -0.004624843597412109\n",
            "\n",
            "-------------------------------------------\n",
            "\n",
            "Training episode 6\n",
            "Episodic Return: -10.558279037475586\n",
            "Episode Length: 124\n",
            "\n",
            "Value Loss: 49.370731353759766\n",
            "Policy Loss: 9.516791343688965\n",
            "Old Approx KL: 0.0031019796151667833\n",
            "Approx KL: 0.0016820827731862664\n",
            "Clip Fraction: 0.022257834800288208\n",
            "Explained Variance: -0.03395068645477295\n",
            "\n",
            "-------------------------------------------\n",
            "\n",
            "Training episode 7\n",
            "Episodic Return: -34.25571060180664\n",
            "Episode Length: 124\n",
            "\n",
            "Value Loss: 209.7178955078125\n",
            "Policy Loss: 16.084104537963867\n",
            "Old Approx KL: -0.018609464168548584\n",
            "Approx KL: 0.0011301810154691339\n",
            "Clip Fraction: 0.10167378919501589\n",
            "Explained Variance: -0.002510666847229004\n",
            "\n",
            "-------------------------------------------\n",
            "\n",
            "Training episode 8\n",
            "Episodic Return: -16.47114372253418\n",
            "Episode Length: 124\n",
            "\n",
            "Value Loss: 28.47936248779297\n",
            "Policy Loss: -5.793796062469482\n",
            "Old Approx KL: 0.004484181758016348\n",
            "Approx KL: 0.0004290267825126648\n",
            "Clip Fraction: 0.030715811965811964\n",
            "Explained Variance: -0.005261778831481934\n",
            "\n",
            "-------------------------------------------\n",
            "\n",
            "Training episode 9\n",
            "Episodic Return: -18.838024139404297\n",
            "Episode Length: 124\n",
            "\n",
            "Value Loss: 28.140167236328125\n",
            "Policy Loss: 0.25380024313926697\n",
            "Old Approx KL: 0.009734313003718853\n",
            "Approx KL: 0.0010048052063211799\n",
            "Clip Fraction: 0.01958689459750795\n",
            "Explained Variance: -0.003638744354248047\n",
            "\n",
            "-------------------------------------------\n",
            "\n",
            "Training episode 10\n",
            "Episodic Return: -1.099348783493042\n",
            "Episode Length: 124\n",
            "\n",
            "Value Loss: 6.525051116943359\n",
            "Policy Loss: -0.3920431137084961\n",
            "Old Approx KL: -0.004360593855381012\n",
            "Approx KL: 0.0005731756682507694\n",
            "Clip Fraction: 0.042289886018659316\n",
            "Explained Variance: -0.035141587257385254\n",
            "\n",
            "-------------------------------------------\n",
            "\n",
            "Training episode 11\n",
            "Episodic Return: 68.54090881347656\n",
            "Episode Length: 124\n",
            "\n",
            "Value Loss: 22.42534065246582\n",
            "Policy Loss: -2.48834228515625\n",
            "Old Approx KL: -0.015720665454864502\n",
            "Approx KL: 0.0015584105858579278\n",
            "Clip Fraction: 0.08057336184458855\n",
            "Explained Variance: 0.0032428503036499023\n",
            "\n",
            "-------------------------------------------\n",
            "\n",
            "Training episode 12\n",
            "Episodic Return: 30.105632781982422\n",
            "Episode Length: 124\n",
            "\n",
            "Value Loss: 93.78115844726562\n",
            "Policy Loss: -10.622076034545898\n",
            "Old Approx KL: 0.0017137328395619988\n",
            "Approx KL: 0.0023276384454220533\n",
            "Clip Fraction: 0.03552350433718445\n",
            "Explained Variance: 0.0017760992050170898\n",
            "\n",
            "-------------------------------------------\n",
            "\n",
            "Training episode 13\n",
            "Episodic Return: 25.564516067504883\n",
            "Episode Length: 124\n",
            "\n",
            "Value Loss: 10.280978202819824\n",
            "Policy Loss: -1.4398813247680664\n",
            "Old Approx KL: 0.006675831973552704\n",
            "Approx KL: 0.0010553350439295173\n",
            "Clip Fraction: 0.1001602564420965\n",
            "Explained Variance: 0.007158935070037842\n",
            "\n",
            "-------------------------------------------\n",
            "\n",
            "Training episode 14\n",
            "Episodic Return: 13.735738754272461\n",
            "Episode Length: 124\n",
            "\n",
            "Value Loss: 16.663820266723633\n",
            "Policy Loss: -2.580988645553589\n",
            "Old Approx KL: -0.000747059762943536\n",
            "Approx KL: 0.0018031150102615356\n",
            "Clip Fraction: 0.14209401712585717\n",
            "Explained Variance: -0.04698038101196289\n",
            "\n",
            "-------------------------------------------\n",
            "\n",
            "Training episode 15\n",
            "Episodic Return: 7.434633731842041\n",
            "Episode Length: 124\n",
            "\n",
            "Value Loss: 132.36192321777344\n",
            "Policy Loss: 15.144989967346191\n",
            "Old Approx KL: 0.003465058980509639\n",
            "Approx KL: 0.0121028246358037\n",
            "Clip Fraction: 0.1587428774398107\n",
            "Explained Variance: 0.015447139739990234\n",
            "\n",
            "-------------------------------------------\n",
            "\n",
            "Training episode 16\n",
            "Episodic Return: 55.009056091308594\n",
            "Episode Length: 124\n",
            "\n",
            "Value Loss: 445.2484130859375\n",
            "Policy Loss: -23.83027458190918\n",
            "Old Approx KL: 0.030390039086341858\n",
            "Approx KL: 0.00385100394487381\n",
            "Clip Fraction: 0.1938212250287716\n",
            "Explained Variance: 0.008212089538574219\n",
            "\n",
            "-------------------------------------------\n",
            "\n",
            "Training episode 17\n",
            "Episodic Return: -18.129138946533203\n",
            "Episode Length: 124\n",
            "\n",
            "Value Loss: 62.56063461303711\n",
            "Policy Loss: 10.399396896362305\n",
            "Old Approx KL: 0.02456226386129856\n",
            "Approx KL: 0.0013778930297121406\n",
            "Clip Fraction: 0.1036324787279989\n",
            "Explained Variance: 0.07411795854568481\n",
            "\n",
            "-------------------------------------------\n",
            "\n",
            "Training episode 18\n",
            "Episodic Return: 24.285707473754883\n",
            "Episode Length: 124\n",
            "\n",
            "Value Loss: 35.01182174682617\n",
            "Policy Loss: -5.404608249664307\n",
            "Old Approx KL: -0.010328044183552265\n",
            "Approx KL: 0.002166385529562831\n",
            "Clip Fraction: 0.11209045589352265\n",
            "Explained Variance: 0.06940197944641113\n",
            "\n",
            "-------------------------------------------\n",
            "\n",
            "Training episode 19\n",
            "Episodic Return: 7.115365505218506\n",
            "Episode Length: 124\n",
            "\n",
            "Value Loss: 136.98654174804688\n",
            "Policy Loss: -11.420315742492676\n",
            "Old Approx KL: 0.007822029292583466\n",
            "Approx KL: 0.002365812659263611\n",
            "Clip Fraction: 0.14761396013518685\n",
            "Explained Variance: -0.0820690393447876\n",
            "\n",
            "-------------------------------------------\n",
            "\n",
            "Training episode 20\n",
            "Episodic Return: 66.52101135253906\n",
            "Episode Length: 124\n",
            "\n",
            "Value Loss: 143.96475219726562\n",
            "Policy Loss: 5.076539516448975\n",
            "Old Approx KL: -0.025510134175419807\n",
            "Approx KL: 0.008794151246547699\n",
            "Clip Fraction: 0.15108618232556897\n",
            "Explained Variance: 0.04145556688308716\n",
            "\n",
            "-------------------------------------------\n",
            "\n",
            "Training episode 21\n",
            "Episodic Return: 42.72393035888672\n",
            "Episode Length: 124\n",
            "\n",
            "Value Loss: 22.966293334960938\n",
            "Policy Loss: -2.269507646560669\n",
            "Old Approx KL: -0.0007290393114089966\n",
            "Approx KL: 0.003982774913311005\n",
            "Clip Fraction: 0.1376424501530635\n",
            "Explained Variance: 0.10201263427734375\n",
            "\n",
            "-------------------------------------------\n",
            "\n",
            "Training episode 22\n",
            "Episodic Return: 4.166665077209473\n",
            "Episode Length: 124\n",
            "\n",
            "Value Loss: 15.632494926452637\n",
            "Policy Loss: -3.6403636932373047\n",
            "Old Approx KL: 0.025148995220661163\n",
            "Approx KL: 0.002323441207408905\n",
            "Clip Fraction: 0.15144230775598788\n",
            "Explained Variance: -0.24441540241241455\n",
            "\n",
            "-------------------------------------------\n",
            "\n",
            "Training episode 23\n",
            "Episodic Return: 62.40777587890625\n",
            "Episode Length: 124\n",
            "\n",
            "Value Loss: 137.05462646484375\n",
            "Policy Loss: -12.563799858093262\n",
            "Old Approx KL: -0.03781668841838837\n",
            "Approx KL: 0.00830011535435915\n",
            "Clip Fraction: 0.24074074078319418\n",
            "Explained Variance: -0.01727008819580078\n",
            "\n",
            "-------------------------------------------\n",
            "\n",
            "Training episode 24\n",
            "Episodic Return: 40.54658126831055\n",
            "Episode Length: 124\n",
            "\n",
            "Value Loss: 14.414938926696777\n",
            "Policy Loss: 1.0446425676345825\n",
            "Old Approx KL: 0.03274654597043991\n",
            "Approx KL: 0.004385086242109537\n",
            "Clip Fraction: 0.12962962973576325\n",
            "Explained Variance: -0.02258133888244629\n",
            "\n",
            "-------------------------------------------\n",
            "\n",
            "Training episode 25\n",
            "Episodic Return: 52.248207092285156\n",
            "Episode Length: 124\n",
            "\n",
            "Value Loss: 29.624574661254883\n",
            "Policy Loss: -3.5979483127593994\n",
            "Old Approx KL: -0.04248563572764397\n",
            "Approx KL: 0.007019648794084787\n",
            "Clip Fraction: 0.22231125347634667\n",
            "Explained Variance: 0.006853699684143066\n",
            "\n",
            "-------------------------------------------\n",
            "\n",
            "Training episode 26\n",
            "Episodic Return: 6.578911781311035\n",
            "Episode Length: 124\n",
            "\n",
            "Value Loss: 141.53646850585938\n",
            "Policy Loss: 15.701587677001953\n",
            "Old Approx KL: 0.06464987248182297\n",
            "Approx KL: 0.014400829561054707\n",
            "Clip Fraction: 0.29585113955868614\n",
            "Explained Variance: -0.02667820453643799\n",
            "\n",
            "-------------------------------------------\n",
            "\n",
            "Training episode 27\n",
            "Episodic Return: 85.18983459472656\n",
            "Episode Length: 124\n",
            "\n",
            "Value Loss: 254.98089599609375\n",
            "Policy Loss: -20.74571990966797\n",
            "Old Approx KL: -0.016097746789455414\n",
            "Approx KL: 0.005256921052932739\n",
            "Clip Fraction: 0.25080128205128205\n",
            "Explained Variance: 0.005183756351470947\n",
            "\n",
            "-------------------------------------------\n",
            "\n",
            "Training episode 28\n",
            "Episodic Return: 48.1870231628418\n",
            "Episode Length: 124\n",
            "\n",
            "Value Loss: 27.875885009765625\n",
            "Policy Loss: -5.580363750457764\n",
            "Old Approx KL: -0.007146542426198721\n",
            "Approx KL: 0.0020069603342562914\n",
            "Clip Fraction: 0.14797008550192556\n",
            "Explained Variance: 0.020915865898132324\n",
            "\n",
            "-------------------------------------------\n",
            "\n",
            "Training episode 29\n",
            "Episodic Return: 44.53974151611328\n",
            "Episode Length: 124\n",
            "\n",
            "Value Loss: 36.75393295288086\n",
            "Policy Loss: 2.4597320556640625\n",
            "Old Approx KL: 0.006263568997383118\n",
            "Approx KL: 0.002889581024646759\n",
            "Clip Fraction: 0.15535968656723315\n",
            "Explained Variance: 0.02196347713470459\n",
            "\n",
            "-------------------------------------------\n",
            "\n",
            "Training episode 30\n",
            "Episodic Return: 66.56983947753906\n",
            "Episode Length: 124\n",
            "\n",
            "Value Loss: 114.5881576538086\n",
            "Policy Loss: -5.347025394439697\n",
            "Old Approx KL: -0.05263941362500191\n",
            "Approx KL: 0.007214084267616272\n",
            "Clip Fraction: 0.2353988603139535\n",
            "Explained Variance: 0.03183811902999878\n",
            "\n",
            "-------------------------------------------\n",
            "\n",
            "Training episode 31\n",
            "Episodic Return: 57.19697189331055\n",
            "Episode Length: 124\n",
            "\n",
            "Value Loss: 69.38805389404297\n",
            "Policy Loss: -7.951174259185791\n",
            "Old Approx KL: 0.015459847636520863\n",
            "Approx KL: 0.0040718489326536655\n",
            "Clip Fraction: 0.24350071233561915\n",
            "Explained Variance: -0.005222320556640625\n",
            "\n",
            "-------------------------------------------\n",
            "\n",
            "Training episode 32\n",
            "Episodic Return: 97.99999237060547\n",
            "Episode Length: 20\n",
            "\n",
            "Value Loss: 427.61151123046875\n",
            "Policy Loss: -10.0400972366333\n",
            "Old Approx KL: 0.0056449249386787415\n",
            "Approx KL: 0.003585636615753174\n",
            "Clip Fraction: 0.16666666666666666\n",
            "Explained Variance: 0.02348947525024414\n",
            "\n",
            "-------------------------------------------\n",
            "\n",
            "Training episode 33\n",
            "Episodic Return: 57.065208435058594\n",
            "Episode Length: 124\n",
            "\n",
            "Value Loss: 34.46354293823242\n",
            "Policy Loss: 0.6702899932861328\n",
            "Old Approx KL: 0.012205876410007477\n",
            "Approx KL: 0.004630714654922485\n",
            "Clip Fraction: 0.20940170952906975\n",
            "Explained Variance: -0.11289036273956299\n",
            "\n",
            "-------------------------------------------\n",
            "\n",
            "Training episode 34\n",
            "Episodic Return: 44.687583923339844\n",
            "Episode Length: 124\n",
            "\n",
            "Value Loss: 53.56973648071289\n",
            "Policy Loss: 9.300230979919434\n",
            "Old Approx KL: 0.040730420500040054\n",
            "Approx KL: 0.002669406356289983\n",
            "Clip Fraction: 0.23379629633874974\n",
            "Explained Variance: -0.009262681007385254\n",
            "\n",
            "-------------------------------------------\n",
            "\n",
            "Training episode 35\n",
            "Episodic Return: 56.532264709472656\n",
            "Episode Length: 124\n",
            "\n",
            "Value Loss: 31.296249389648438\n",
            "Policy Loss: -4.837834358215332\n",
            "Old Approx KL: 0.0014372318983078003\n",
            "Approx KL: 0.0017155110836029053\n",
            "Clip Fraction: 0.1344373219797754\n",
            "Explained Variance: -0.016486763954162598\n",
            "\n",
            "-------------------------------------------\n",
            "\n",
            "Training episode 36\n",
            "Episodic Return: 80.30303955078125\n",
            "Episode Length: 124\n",
            "\n",
            "Value Loss: 63.5145378112793\n",
            "Policy Loss: -10.036117553710938\n",
            "Old Approx KL: -0.03562118858098984\n",
            "Approx KL: 0.0037207778077572584\n",
            "Clip Fraction: 0.1907941595547729\n",
            "Explained Variance: -0.003974556922912598\n",
            "\n",
            "-------------------------------------------\n",
            "\n",
            "Training episode 37\n",
            "Episodic Return: 58.9286003112793\n",
            "Episode Length: 124\n",
            "\n",
            "Value Loss: 114.15194702148438\n",
            "Policy Loss: 13.80133056640625\n",
            "Old Approx KL: 0.001587552367709577\n",
            "Approx KL: 0.0023265208583325148\n",
            "Clip Fraction: 0.22337962973576325\n",
            "Explained Variance: -0.028672099113464355\n",
            "\n",
            "-------------------------------------------\n",
            "\n",
            "Training episode 38\n",
            "Episodic Return: 87.70001983642578\n",
            "Episode Length: 123\n",
            "\n",
            "Value Loss: 39.4914436340332\n",
            "Policy Loss: -4.946458339691162\n",
            "Old Approx KL: -0.0074243503622710705\n",
            "Approx KL: 0.010203229263424873\n",
            "Clip Fraction: 0.1638049452095969\n",
            "Explained Variance: -0.014130234718322754\n",
            "\n",
            "-------------------------------------------\n",
            "\n",
            "Training episode 39\n",
            "Episodic Return: 43.838016510009766\n",
            "Episode Length: 124\n",
            "\n",
            "Value Loss: 40.30927276611328\n",
            "Policy Loss: 0.8846274018287659\n",
            "Old Approx KL: -0.006731629371643066\n",
            "Approx KL: 0.005283867474645376\n",
            "Clip Fraction: 0.19088319084073743\n",
            "Explained Variance: -0.0005830526351928711\n",
            "\n",
            "-------------------------------------------\n",
            "\n",
            "Training episode 40\n",
            "Episodic Return: 50.29864501953125\n",
            "Episode Length: 124\n",
            "\n",
            "Value Loss: 20.601776123046875\n",
            "Policy Loss: -3.0148136615753174\n",
            "Old Approx KL: -0.005006194114685059\n",
            "Approx KL: 0.002018864033743739\n",
            "Clip Fraction: 0.21180555555555555\n",
            "Explained Variance: -0.013526558876037598\n",
            "\n",
            "-------------------------------------------\n",
            "\n",
            "Training episode 41\n",
            "Episodic Return: 94.90003204345703\n",
            "Episode Length: 51\n",
            "\n",
            "Value Loss: 97.13003540039062\n",
            "Policy Loss: -13.47117805480957\n",
            "Old Approx KL: -0.06549204140901566\n",
            "Approx KL: 0.007737656589597464\n",
            "Clip Fraction: 0.3213975702722867\n",
            "Explained Variance: -0.02633380889892578\n",
            "\n",
            "-------------------------------------------\n",
            "\n",
            "Training episode 42\n",
            "Episodic Return: 90.0999984741211\n",
            "Episode Length: 99\n",
            "\n",
            "Value Loss: 43.93344497680664\n",
            "Policy Loss: 6.379073143005371\n",
            "Old Approx KL: 0.05442686006426811\n",
            "Approx KL: 0.015734052285552025\n",
            "Clip Fraction: 0.3542562726364341\n",
            "Explained Variance: -0.00564122200012207\n",
            "\n",
            "-------------------------------------------\n",
            "\n",
            "Training episode 43\n",
            "Episodic Return: 87.70005798339844\n",
            "Episode Length: 123\n",
            "\n",
            "Value Loss: 9.983779907226562\n",
            "Policy Loss: -1.031559705734253\n",
            "Old Approx KL: -0.018872933462262154\n",
            "Approx KL: 0.002118902513757348\n",
            "Clip Fraction: 0.22935744820751697\n",
            "Explained Variance: 0.0011161565780639648\n",
            "\n",
            "-------------------------------------------\n",
            "\n",
            "Training episode 44\n",
            "Episodic Return: 87.70000457763672\n",
            "Episode Length: 123\n",
            "\n",
            "Value Loss: 40.584877014160156\n",
            "Policy Loss: -0.5051494240760803\n",
            "Old Approx KL: -0.003771041054278612\n",
            "Approx KL: 0.00695645809173584\n",
            "Clip Fraction: 0.2847985350168668\n",
            "Explained Variance: -0.014820337295532227\n",
            "\n",
            "-------------------------------------------\n",
            "\n",
            "Training episode 45\n",
            "Episodic Return: 87.79998779296875\n",
            "Episode Length: 122\n",
            "\n",
            "Value Loss: 7.883904933929443\n",
            "Policy Loss: -0.7541717290878296\n",
            "Old Approx KL: 0.08705508708953857\n",
            "Approx KL: 0.007444053888320923\n",
            "Clip Fraction: 0.39342948717948717\n",
            "Explained Variance: -0.0002541542053222656\n",
            "\n",
            "-------------------------------------------\n",
            "\n",
            "Training episode 46\n",
            "Episodic Return: 85.60133361816406\n",
            "Episode Length: 124\n",
            "\n",
            "Value Loss: 146.25254821777344\n",
            "Policy Loss: 6.44042444229126\n",
            "Old Approx KL: 0.08805812150239944\n",
            "Approx KL: 0.01955539919435978\n",
            "Clip Fraction: 0.39476495726495725\n",
            "Explained Variance: 0.0013516545295715332\n",
            "\n",
            "-------------------------------------------\n",
            "\n",
            "Training episode 47\n",
            "Episodic Return: 89.1999740600586\n",
            "Episode Length: 108\n",
            "\n",
            "Value Loss: 44.65032958984375\n",
            "Policy Loss: -7.470102787017822\n",
            "Old Approx KL: 0.018352381885051727\n",
            "Approx KL: 0.013980448246002197\n",
            "Clip Fraction: 0.35222630699475604\n",
            "Explained Variance: -0.006666064262390137\n",
            "\n",
            "-------------------------------------------\n",
            "\n",
            "Training episode 48\n",
            "Episodic Return: 90.70001220703125\n",
            "Episode Length: 93\n",
            "\n",
            "Value Loss: 45.36487579345703\n",
            "Policy Loss: -10.55203914642334\n",
            "Old Approx KL: -0.0997464656829834\n",
            "Approx KL: 0.0051454901695251465\n",
            "Clip Fraction: 0.4684027777777778\n",
            "Explained Variance: -0.0040740966796875\n",
            "\n",
            "-------------------------------------------\n",
            "\n",
            "Training episode 49\n",
            "Episodic Return: 90.49998474121094\n",
            "Episode Length: 95\n",
            "\n",
            "Value Loss: 86.90275573730469\n",
            "Policy Loss: 9.162793159484863\n",
            "Old Approx KL: 0.017089514061808586\n",
            "Approx KL: 0.005908811464905739\n",
            "Clip Fraction: 0.38554292950365276\n",
            "Explained Variance: 9.417533874511719e-06\n",
            "\n",
            "-------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from supersuit import color_reduction_v0, frame_stack_v1, resize_v1\n",
        "from torch.distributions.categorical import Categorical\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "from pettingzoo.butterfly import pistonball_v6\n",
        "\n",
        "\n",
        "writer = SummaryWriter(\"runs/ppo_pistonball_demo\")\n",
        "## python ppo_pistonball_demo.py\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self, num_actions):\n",
        "        super().__init__()\n",
        "\n",
        "        self.network = nn.Sequential(\n",
        "            self._layer_init(nn.Conv2d(4, 32, 3, padding=1)),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.ReLU(),\n",
        "            self._layer_init(nn.Conv2d(32, 64, 3, padding=1)),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.ReLU(),\n",
        "            self._layer_init(nn.Conv2d(64, 128, 3, padding=1)),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            self._layer_init(nn.Linear(128 * 8 * 8, 512)),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.actor = self._layer_init(nn.Linear(512, num_actions), std=0.01)\n",
        "        self.critic = self._layer_init(nn.Linear(512, 1))\n",
        "\n",
        "    def _layer_init(self, layer, std=np.sqrt(2), bias_const=0.0):\n",
        "        torch.nn.init.orthogonal_(layer.weight, std)\n",
        "        torch.nn.init.constant_(layer.bias, bias_const)\n",
        "        return layer\n",
        "\n",
        "    def get_value(self, x):\n",
        "        return self.critic(self.network(x / 255.0))\n",
        "\n",
        "    def get_action_and_value(self, x, action=None):\n",
        "        hidden = self.network(x / 255.0)\n",
        "        logits = self.actor(hidden)\n",
        "        probs = Categorical(logits=logits)\n",
        "        if action is None:\n",
        "            action = probs.sample()\n",
        "        return action, probs.log_prob(action), probs.entropy(), self.critic(hidden)\n",
        "\n",
        "\n",
        "def batchify_obs(obs, device):\n",
        "    \"\"\"Converts PZ style observations to batch of torch arrays.\"\"\"\n",
        "    # convert to list of np arrays\n",
        "    obs = np.stack([obs[a] for a in obs], axis=0)\n",
        "    # transpose to be (batch, channel, height, width)\n",
        "    obs = obs.transpose(0, -1, 1, 2)\n",
        "    # convert to torch\n",
        "    obs = torch.tensor(obs).to(device)\n",
        "\n",
        "    return obs\n",
        "\n",
        "\n",
        "def batchify(x, device):\n",
        "    \"\"\"Converts PZ style returns to batch of torch arrays.\"\"\"\n",
        "    # convert to list of np arrays\n",
        "    x = np.stack([x[a] for a in x], axis=0)\n",
        "    # convert to torch\n",
        "    x = torch.tensor(x).to(device)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def unbatchify(x, env):\n",
        "    \"\"\"Converts np array to PZ style arguments.\"\"\"\n",
        "    x = x.cpu().numpy()\n",
        "    x = {a: x[i] for i, a in enumerate(env.possible_agents)}\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    \"\"\"ALGO PARAMS\"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    ent_coef = 0.1\n",
        "    vf_coef = 0.1\n",
        "    clip_coef = 0.1\n",
        "    gamma = 0.99\n",
        "    batch_size = 32\n",
        "    stack_size = 4\n",
        "    frame_size = (64, 64)\n",
        "    max_cycles = 125\n",
        "    total_episodes = 50\n",
        "\n",
        "    \"\"\" ENV SETUP \"\"\"\n",
        "    env = pistonball_v6.parallel_env(\n",
        "        render_mode=\"rgb_array\", continuous=False, max_cycles=max_cycles,n_pistons=10\n",
        "    )\n",
        "    env = color_reduction_v0(env)\n",
        "    env = resize_v1(env, frame_size[0], frame_size[1])\n",
        "    env = frame_stack_v1(env, stack_size=stack_size)\n",
        "    num_agents = len(env.possible_agents)\n",
        "    num_actions = env.action_space(env.possible_agents[0]).n\n",
        "    observation_size = env.observation_space(env.possible_agents[0]).shape\n",
        "\n",
        "    \"\"\" LEARNER SETUP \"\"\"\n",
        "    agent = Agent(num_actions=num_actions).to(device)\n",
        "    optimizer = optim.Adam(agent.parameters(), lr=0.001, eps=1e-5)\n",
        "\n",
        "    \"\"\" ALGO LOGIC: EPISODE STORAGE\"\"\"\n",
        "    end_step = 0\n",
        "    total_episodic_return = 0\n",
        "    rb_obs = torch.zeros((max_cycles, num_agents, stack_size, *frame_size)).to(device)\n",
        "    rb_actions = torch.zeros((max_cycles, num_agents)).to(device)\n",
        "    rb_logprobs = torch.zeros((max_cycles, num_agents)).to(device)\n",
        "    rb_rewards = torch.zeros((max_cycles, num_agents)).to(device)\n",
        "    rb_terms = torch.zeros((max_cycles, num_agents)).to(device)\n",
        "    rb_values = torch.zeros((max_cycles, num_agents)).to(device)\n",
        "\n",
        "    \"\"\" TRAINING LOGIC \"\"\"\n",
        "    # train for n number of episodes\n",
        "    for episode in range(total_episodes):\n",
        "\n",
        "        # collect an episode\n",
        "        with torch.no_grad():\n",
        "            # collect observations and convert to batch of torch tensors\n",
        "            next_obs = env.reset(seed=None)\n",
        "            # reset the episodic return\n",
        "            total_episodic_return = 0\n",
        "\n",
        "            # each episode has num_steps\n",
        "            for step in range(0, max_cycles):\n",
        "                # rollover the observation\n",
        "                obs = batchify_obs(next_obs, device)\n",
        "\n",
        "                # get action from the agent\n",
        "                actions, logprobs, _, values = agent.get_action_and_value(obs)\n",
        "\n",
        "                # execute the environment and log data\n",
        "                next_obs, rewards, terms, truncs, infos = env.step(\n",
        "                    unbatchify(actions, env)\n",
        "                )\n",
        "\n",
        "                # add to episode storage\n",
        "                rb_obs[step] = obs\n",
        "                rb_rewards[step] = batchify(rewards, device)\n",
        "                rb_terms[step] = batchify(terms, device)\n",
        "                rb_actions[step] = actions\n",
        "                rb_logprobs[step] = logprobs\n",
        "                rb_values[step] = values.flatten()\n",
        "\n",
        "                # compute episodic return\n",
        "                total_episodic_return += rb_rewards[step].cpu().numpy()\n",
        "\n",
        "                # if we reach termination or truncation, end\n",
        "                if any([terms[a] for a in terms]) or any([truncs[a] for a in truncs]):\n",
        "                    end_step = step\n",
        "                    break\n",
        "\n",
        "        # bootstrap value if not done\n",
        "        with torch.no_grad():\n",
        "            rb_advantages = torch.zeros_like(rb_rewards).to(device)\n",
        "            for t in reversed(range(end_step)):\n",
        "                delta = (\n",
        "                    rb_rewards[t]\n",
        "                    + gamma * rb_values[t + 1] * rb_terms[t + 1]\n",
        "                    - rb_values[t]\n",
        "                )\n",
        "                rb_advantages[t] = delta + gamma * gamma * rb_advantages[t + 1]\n",
        "            rb_returns = rb_advantages + rb_values\n",
        "\n",
        "        # convert our episodes to batch of individual transitions\n",
        "        b_obs = torch.flatten(rb_obs[:end_step], start_dim=0, end_dim=1)\n",
        "        b_logprobs = torch.flatten(rb_logprobs[:end_step], start_dim=0, end_dim=1)\n",
        "        b_actions = torch.flatten(rb_actions[:end_step], start_dim=0, end_dim=1)\n",
        "        b_returns = torch.flatten(rb_returns[:end_step], start_dim=0, end_dim=1)\n",
        "        b_values = torch.flatten(rb_values[:end_step], start_dim=0, end_dim=1)\n",
        "        b_advantages = torch.flatten(rb_advantages[:end_step], start_dim=0, end_dim=1)\n",
        "\n",
        "        # Optimizing the policy and value network\n",
        "        b_index = np.arange(len(b_obs))\n",
        "        clip_fracs = []\n",
        "        for repeat in range(3):\n",
        "            # shuffle the indices we use to access the data\n",
        "            np.random.shuffle(b_index)\n",
        "            for start in range(0, len(b_obs), batch_size):\n",
        "                # select the indices we want to train on\n",
        "                end = start + batch_size\n",
        "                batch_index = b_index[start:end]\n",
        "\n",
        "                _, newlogprob, entropy, value = agent.get_action_and_value(\n",
        "                    b_obs[batch_index], b_actions.long()[batch_index]\n",
        "                )\n",
        "                logratio = newlogprob - b_logprobs[batch_index]\n",
        "                ratio = logratio.exp()\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    # calculate approx_kl\n",
        "                    old_approx_kl = (-logratio).mean()\n",
        "                    approx_kl = ((ratio - 1) - logratio).mean()\n",
        "                    clip_fracs += [\n",
        "                        ((ratio - 1.0).abs() > clip_coef).float().mean().item()\n",
        "                    ]\n",
        "\n",
        "                # normalize advantaegs\n",
        "                advantages = b_advantages[batch_index]\n",
        "                advantages = (advantages - advantages.mean()) / (\n",
        "                    advantages.std() + 1e-8\n",
        "                )\n",
        "\n",
        "                # Policy loss\n",
        "                pg_loss1 = -b_advantages[batch_index] * ratio\n",
        "                pg_loss2 = -b_advantages[batch_index] * torch.clamp(\n",
        "                    ratio, 1 - clip_coef, 1 + clip_coef\n",
        "                )\n",
        "                pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
        "\n",
        "                # Value loss\n",
        "                value = value.flatten()\n",
        "                v_loss_unclipped = (value - b_returns[batch_index]) ** 2\n",
        "                v_clipped = b_values[batch_index] + torch.clamp(\n",
        "                    value - b_values[batch_index],\n",
        "                    -clip_coef,\n",
        "                    clip_coef,\n",
        "                )\n",
        "                v_loss_clipped = (v_clipped - b_returns[batch_index]) ** 2\n",
        "                v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
        "                v_loss = 0.5 * v_loss_max.mean()\n",
        "\n",
        "                entropy_loss = entropy.mean()\n",
        "                loss = pg_loss - ent_coef * entropy_loss + v_loss * vf_coef\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                \n",
        "\n",
        "                optimizer.step()\n",
        "        \n",
        "        # add  tensorboard logs\n",
        "\n",
        "        writer.add_scalar(\"value_loss\", v_loss.item(),episode)\n",
        "        writer.add_scalar(\"policy_loss\", pg_loss.item(),episode)\n",
        "        writer.add_scalar(\"value_loss\", v_loss.item(),episode)\n",
        "        writer.add_scalar(\"episod_length\", end_step,episode)\n",
        "        writer.add_scalar(\"episodic_return\", np.mean(total_episodic_return),episode)\n",
        "\n",
        "        y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n",
        "        var_y = np.var(y_true)\n",
        "        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
        "\n",
        "        print(f\"Training episode {episode}\")\n",
        "        print(f\"Episodic Return: {np.mean(total_episodic_return)}\")\n",
        "        print(f\"Episode Length: {end_step}\")\n",
        "        print(\"\")\n",
        "        print(f\"Value Loss: {v_loss.item()}\")\n",
        "        print(f\"Policy Loss: {pg_loss.item()}\")\n",
        "        print(f\"Old Approx KL: {old_approx_kl.item()}\")\n",
        "        print(f\"Approx KL: {approx_kl.item()}\")\n",
        "        print(f\"Clip Fraction: {np.mean(clip_fracs)}\")\n",
        "        print(f\"Explained Variance: {explained_var.item()}\")\n",
        "        print(\"\\n-------------------------------------------\\n\")\n",
        "\n",
        "    \"\"\" RENDER THE POLICY \"\"\"\n",
        "    env = pistonball_v6.parallel_env(render_mode=\"rgb_array\", continuous=False,n_pistons=10)\n",
        "    env = color_reduction_v0(env)\n",
        "    env = resize_v1(env, 64, 64)\n",
        "    env = frame_stack_v1(env, stack_size=4)\n",
        "\n",
        "    agent.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # render 5 episodes out\n",
        "        for episode in range(5):\n",
        "            obs = batchify_obs(env.reset(seed=None), device)\n",
        "            terms = [False]\n",
        "            truncs = [False]\n",
        "            while not any(terms) and not any(truncs):\n",
        "                actions, logprobs, _, values = agent.get_action_and_value(obs)\n",
        "                obs, rewards, terms, truncs, infos = env.step(unbatchify(actions, env))\n",
        "                obs = batchify_obs(obs, device)\n",
        "                terms = [terms[a] for a in terms]\n",
        "                truncs = [truncs[a] for a in truncs]"
      ]
    }
  ]
}